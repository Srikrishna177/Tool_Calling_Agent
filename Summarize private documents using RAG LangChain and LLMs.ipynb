{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Summarize Private Documents Using RAG, LangChain, and LLMs**\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Imagine it's your first day at an exciting new job at a fast-growing tech company, Innovatech. You're filled with a mix of anticipation and nerves, eager to make a great first impression and contribute to your team. As you find your way to your desk, decorated with a welcoming note and some company swag, you can't help but feel a surge of pride. This is the moment you've been working towards, and it's finally here.\n",
    "\n",
    "Your manager, Alex, greets you with a warm smile. \"Welcome aboard! We're thrilled to have you with us. I have sent you a folder. Inside this folder, you'll find everything you need to get up to speed on our company policies, culture, and the projects your team is working on. Please keep them private.\"\n",
    "\n",
    "You thank Alex and open the folder, only to be greeted by a mountain of documents - manuals, guidelines, technical documents, project summaries, and more. It's overwhelming. You think to yourself, \"How am I supposed to absorb all of this information in a short time? And they are private and I cannot just upload it to GPT to summarize them.\" \"Why not create an agent to read and summarize them for you, and then you can just ask it?\" your colleague, Jordan, suggests with an encouraging grin. You're intrigued, but uncertain; the world of large language models (LLMs) is one that you've only scratched the surface of. Sensing your hesitation, Jordan elaborates, \"Imagine having a personal assistant who's not only exceptionally fast at reading but can also understand and condense the information into easy-to-digest summaries. That's what an LLM can do for you, especially when enhanced with LangChain and Retrieval-Augmented Generation (RAG) technology.\"  \n",
    "\n",
    "\"But how do I get started? And how long will it take to set up something like that?\" you ask. Jordan says, \"Let's dive into a project that will not only help you tackle this immediate challenge but also equip you with a skill set that's becoming indispensable in this field.\"\n",
    "\n",
    "So, this project steps you through the fascinating world of LLMs and RAG, starting from the basics of what these technologies are, to building a practical application that can read and summarize documents for you. By the end of this tutorial, you have a working tool capable of processing the pile of documents on your desk, allowing you to focus on making meaningful contributions to your projects sooner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, you are going to use the following libraries:\n",
    "\n",
    "*   [`ibm-watsonx-ai`](https://ibm.github.io/watson-machine-learning-sdk/index.html) for using LLMs from IBM's watsonx.ai\n",
    "*   [`LangChain`](https://www.langchain.com/) for using its different chain and prompt functions\n",
    "*   [`Hugging Face`](https://huggingface.co/models?other=embeddings) and [`Hugging Face Hub`](https://huggingface.co/models?other=embeddings) for their embedding methods for processing text data\n",
    "*   [`SentenceTransformers`](https://www.sbert.net/) for transforming sentences into high-dimensional vectors\n",
    "*   [`Chroma DB`](https://www.trychroma.com/) for efficient storage and retrieval of high-dimensional text vector data\n",
    "*   [`wget`](https://pypi.org/project/wget/) for downloading files from remote systems\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T13:02:02.672218Z",
     "start_time": "2025-07-22T13:01:38.109929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "!pip install --user \"ibm-watsonx-ai==0.2.6\"\n",
    "!pip install --user \"langchain-ibm==0.1.4\"\n",
    "!pip install --user \"huggingface == 0.0.1\"\n",
    "!pip install --user \"huggingface-hub == 0.23.4\"\n",
    "!pip install --user \"sentence-transformers == 2.5.1\"\n",
    "!pip install --user \"chromadb\"\n",
    "!pip install --user \"wget == 3.2\"\n",
    "!pip install -U langchain-community\n",
    "!pip install langchain-text-splitters\n"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T13:02:15.289502Z",
     "start_time": "2025-07-22T13:02:08.584959Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install -U langchain langchain-core langchain-community langchain-text-splitters",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in e:\\rag_1\\.venv\\lib\\site-packages (0.3.26)\n",
      "Requirement already satisfied: langchain-core in e:\\rag_1\\.venv\\lib\\site-packages (0.3.70)\n",
      "Requirement already satisfied: langchain-community in e:\\rag_1\\.venv\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-text-splitters in e:\\rag_1\\.venv\\lib\\site-packages (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in e:\\rag_1\\.venv\\lib\\site-packages (from langchain) (0.4.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in e:\\rag_1\\.venv\\lib\\site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in e:\\rag_1\\.venv\\lib\\site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in e:\\rag_1\\.venv\\lib\\site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in e:\\rag_1\\.venv\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in e:\\rag_1\\.venv\\lib\\site-packages (from langchain-core) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in e:\\rag_1\\.venv\\lib\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in e:\\rag_1\\.venv\\lib\\site-packages (from langchain-core) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in e:\\rag_1\\.venv\\lib\\site-packages (from langchain-core) (23.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in e:\\rag_1\\.venv\\lib\\site-packages (from langchain-community) (3.12.14)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in e:\\rag_1\\.venv\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in e:\\rag_1\\.venv\\lib\\site-packages (from langchain-community) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in e:\\rag_1\\.venv\\lib\\site-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in e:\\rag_1\\.venv\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in e:\\rag_1\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in e:\\rag_1\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in e:\\rag_1\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in e:\\rag_1\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in e:\\rag_1\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in e:\\rag_1\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in e:\\rag_1\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in e:\\rag_1\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in e:\\rag_1\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in e:\\rag_1\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in e:\\rag_1\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in e:\\rag_1\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (3.11.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in e:\\rag_1\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in e:\\rag_1\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in e:\\rag_1\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in e:\\rag_1\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in e:\\rag_1\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in e:\\rag_1\\.venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\rag_1\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\rag_1\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\rag_1\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\rag_1\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2025.7.14)\n",
      "Requirement already satisfied: greenlet>=1 in e:\\rag_1\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
      "Requirement already satisfied: anyio in e:\\rag_1\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\rag_1\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in e:\\rag_1\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in e:\\rag_1\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in e:\\rag_1\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T13:02:16.738033Z",
     "start_time": "2025-07-22T13:02:15.291032Z"
    }
   },
   "source": [
    "# You can use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.chains import RetrievalQA\n",
    "from langchain_community.chains import ConversationalRetrievalChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.memory import ConversationBufferMemory\n",
    "from langchain_core.runnables.config import set_config_context\n",
    "\n",
    "\n",
    "from ibm_watsonx_ai.foundation_models import Model\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes, DecodingMethods\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "import wget"
   ],
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'set_config_context' from 'langchain_core.runnables.config' (E:\\RAG_1\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[24]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      8\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_community\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdocument_loaders\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m TextLoader\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_text_splitters\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m CharacterTextSplitter\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_community\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mvectorstores\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Chroma\n\u001B[32m     11\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_community\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01membeddings\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m HuggingFaceEmbeddings\n\u001B[32m     12\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_community\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mchains\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m RetrievalQA\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\RAG_1\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\__init__.py:529\u001B[39m, in \u001B[36m__getattr__\u001B[39m\u001B[34m(name)\u001B[39m\n\u001B[32m    527\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__getattr__\u001B[39m(name: \u001B[38;5;28mstr\u001B[39m) -> Any:\n\u001B[32m    528\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m _module_lookup:\n\u001B[32m--> \u001B[39m\u001B[32m529\u001B[39m         module = \u001B[43mimportlib\u001B[49m\u001B[43m.\u001B[49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_module_lookup\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    530\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(module, name)\n\u001B[32m    531\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmodule \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m has no attribute \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001B[39m, in \u001B[36mimport_module\u001B[39m\u001B[34m(name, package)\u001B[39m\n\u001B[32m     88\u001B[39m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m     89\u001B[39m         level += \u001B[32m1\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m90\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\RAG_1\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:23\u001B[39m\n\u001B[32m     21\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01membeddings\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Embeddings\n\u001B[32m     22\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m xor_args\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mvectorstores\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m VectorStore\n\u001B[32m     25\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_community\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mvectorstores\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m maximal_marginal_relevance\n\u001B[32m     27\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m TYPE_CHECKING:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\RAG_1\\.venv\\Lib\\site-packages\\langchain_core\\vectorstores\\__init__.py:28\u001B[39m, in \u001B[36m__getattr__\u001B[39m\u001B[34m(attr_name)\u001B[39m\n\u001B[32m     26\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__getattr__\u001B[39m(attr_name: \u001B[38;5;28mstr\u001B[39m) -> \u001B[38;5;28mobject\u001B[39m:\n\u001B[32m     27\u001B[39m     module_name = _dynamic_imports.get(attr_name)\n\u001B[32m---> \u001B[39m\u001B[32m28\u001B[39m     result = \u001B[43mimport_attr\u001B[49m\u001B[43m(\u001B[49m\u001B[43mattr_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodule_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m__spec__\u001B[49m\u001B[43m.\u001B[49m\u001B[43mparent\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     29\u001B[39m     \u001B[38;5;28mglobals\u001B[39m()[attr_name] = result\n\u001B[32m     30\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\RAG_1\\.venv\\Lib\\site-packages\\langchain_core\\_import_utils.py:29\u001B[39m, in \u001B[36mimport_attr\u001B[39m\u001B[34m(attr_name, module_name, package)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001B[39m, in \u001B[36mimport_module\u001B[39m\u001B[34m(name, package)\u001B[39m\n\u001B[32m     88\u001B[39m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m     89\u001B[39m         level += \u001B[32m1\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m90\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\RAG_1\\.venv\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:42\u001B[39m\n\u001B[32m     39\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtyping_extensions\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Self, override\n\u001B[32m     41\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01membeddings\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Embeddings\n\u001B[32m---> \u001B[39m\u001B[32m42\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mretrievers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m BaseRetriever, LangSmithRetrieverParams\n\u001B[32m     43\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mrunnables\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mconfig\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m run_in_executor\n\u001B[32m     45\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m TYPE_CHECKING:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\RAG_1\\.venv\\Lib\\site-packages\\langchain_core\\retrievers.py:35\u001B[39m\n\u001B[32m     33\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcallbacks\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Callbacks\n\u001B[32m     34\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdocuments\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Document\n\u001B[32m---> \u001B[39m\u001B[32m35\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mrunnables\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     36\u001B[39m     Runnable,\n\u001B[32m     37\u001B[39m     RunnableConfig,\n\u001B[32m     38\u001B[39m     RunnableSerializable,\n\u001B[32m     39\u001B[39m     ensure_config,\n\u001B[32m     40\u001B[39m )\n\u001B[32m     41\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mrunnables\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mconfig\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m run_in_executor\n\u001B[32m     43\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m TYPE_CHECKING:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\RAG_1\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\__init__.py:129\u001B[39m, in \u001B[36m__getattr__\u001B[39m\u001B[34m(attr_name)\u001B[39m\n\u001B[32m    127\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__getattr__\u001B[39m(attr_name: \u001B[38;5;28mstr\u001B[39m) -> \u001B[38;5;28mobject\u001B[39m:\n\u001B[32m    128\u001B[39m     module_name = _dynamic_imports.get(attr_name)\n\u001B[32m--> \u001B[39m\u001B[32m129\u001B[39m     result = \u001B[43mimport_attr\u001B[49m\u001B[43m(\u001B[49m\u001B[43mattr_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodule_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m__spec__\u001B[49m\u001B[43m.\u001B[49m\u001B[43mparent\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    130\u001B[39m     \u001B[38;5;28mglobals\u001B[39m()[attr_name] = result\n\u001B[32m    131\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\RAG_1\\.venv\\Lib\\site-packages\\langchain_core\\_import_utils.py:29\u001B[39m, in \u001B[36mimport_attr\u001B[39m\u001B[34m(attr_name, module_name, package)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001B[39m, in \u001B[36mimport_module\u001B[39m\u001B[34m(name, package)\u001B[39m\n\u001B[32m     88\u001B[39m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m     89\u001B[39m         level += \u001B[32m1\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m90\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\RAG_1\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:49\u001B[39m\n\u001B[32m     43\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_api\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m beta_decorator\n\u001B[32m     44\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mload\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mserializable\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     45\u001B[39m     Serializable,\n\u001B[32m     46\u001B[39m     SerializedConstructor,\n\u001B[32m     47\u001B[39m     SerializedNotImplemented,\n\u001B[32m     48\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m49\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mrunnables\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mconfig\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     50\u001B[39m     RunnableConfig,\n\u001B[32m     51\u001B[39m     acall_func_with_variable_args,\n\u001B[32m     52\u001B[39m     call_func_with_variable_args,\n\u001B[32m     53\u001B[39m     ensure_config,\n\u001B[32m     54\u001B[39m     get_async_callback_manager_for_config,\n\u001B[32m     55\u001B[39m     get_callback_manager_for_config,\n\u001B[32m     56\u001B[39m     get_config_list,\n\u001B[32m     57\u001B[39m     get_executor_for_config,\n\u001B[32m     58\u001B[39m     merge_configs,\n\u001B[32m     59\u001B[39m     patch_config,\n\u001B[32m     60\u001B[39m     run_in_executor,\n\u001B[32m     61\u001B[39m     set_config_context,\n\u001B[32m     62\u001B[39m )\n\u001B[32m     63\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mrunnables\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mgraph\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Graph\n\u001B[32m     64\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mrunnables\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     65\u001B[39m     AddableDict,\n\u001B[32m     66\u001B[39m     AnyConfigurableField,\n\u001B[32m   (...)\u001B[39m\u001B[32m     82\u001B[39m     is_async_generator,\n\u001B[32m     83\u001B[39m )\n",
      "\u001B[31mImportError\u001B[39m: cannot import name 'set_config_context' from 'langchain_core.runnables.config' (E:\\RAG_1\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py)"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "### Load the document\n",
    "\n",
    "The document, which is provided in a TXT format, outlines some company policies and serves as an example data set for the project.\n",
    "\n",
    "This is the `load` step in `Indexing`.<br>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/MPdUH7bXpHR5muZztZfOQg.png\" width=\"50%\" alt=\"split\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'companyPolicies.txt'\nurl = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/6JDbUb_L3egv_eOkouY71A.txt'\n\n# Use wget to download the file\nwget.download(url, out=filename)\nprint('file downloaded')"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the file is downloaded and imported into this lab environment, you can use the following code to look at the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, 'r') as file:\n    # Read the contents of the file\n    contents = file.read()\n    print(contents)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the content, you see that the document discusses nine fundamental policies within a company.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the document into chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, you are splitting the document into chunks, which is basically the `split` process in `Indexing`.\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/0JFmAV5e_mejAXvCilgHWg.png\" width=\"50%\" alt=\"split\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LangChain` is used to split the document and create chunks. It helps you divide a long story (document) into smaller parts, which are called `chunks`, so that it's easier to handle. \n",
    "\n",
    "For the splitting process, the goal is to ensure that each segment is as extensive as if you were to count to a certain number of characters and meet the split separator. This certain number is called `chunk size`. Let's set 1000 as the chunk size in this project. Though the chunk size is 1000, the splitting is happening randomly. This is an issue with LangChain. `CharacterTextSplitter` uses `\\n\\n` as the default split separator. You can change it by adding the `separator` parameter in the `CharacterTextSplitter` function; for example, `separator=\"\\n\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(filename)\ndocuments = loader.load()\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ntexts = text_splitter.split_documents(documents)\nprint(len(texts))"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the ouput of print, you see that the document has been split into 16 chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding and storing\n",
    "This step is the `embed` and `store` processes in `Indexing`. <br>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/u_oJz3v2cSR_lr0YvU6PaA.png\" width=\"50%\" alt=\"split\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, you're taking the pieces of the story, your \"chunks,\" converting the text into numbers, and making them easier for your computer to understand and remember by using a process called \"embedding.\" Think of embedding like giving each chunk its own special code. This code helps the computer quickly find and recognize each chunk later on. \n",
    "\n",
    "You do this embedding process during a phase called \"Indexing.\" The reason why is to make sure that when you need to find specific information or details within your larger document, the computer can do so swiftly and accurately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates a default embedding model from Hugging Face and ingests them to Chromadb.\n",
    "\n",
    "When it's completed, print \"document ingested\".\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings()\ndocsearch = Chroma.from_documents(texts, embeddings)  # store the embedding in docsearch using Chromadb\nprint('document ingested')"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point, you've been performing the `Indexing` task. The next step is the `Retrieval` task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM model construction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you'll build an LLM model from IBM watsonx.ai. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define a model ID and choose which model you want to use. There are many other model options. Refer to [Foundation Models](https://ibm.github.io/watsonx-ai-python-sdk/foundation_models.html) for other model options. This tutorial uses the `FLAN_UL2` model as an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'google/flan-ul2'"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define parameters for the model.\n",
    "\n",
    "The decoding method is set to `greedy` to get a deterministic output.\n",
    "\n",
    "For other commonly used parameters, you can refer to [Foundation model parameters: decoding and stopping criteria](https://www.ibm.com/docs/en/watsonx-as-a-service?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-RAG_v1_1711546843&topic=lab-model-parameters-prompting).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,  \n    GenParams.MIN_NEW_TOKENS: 130, # this controls the minimum number of tokens in the generated output\n    GenParams.MAX_NEW_TOKENS: 256,  # this controls the maximum number of tokens in the generated output\n    GenParams.TEMPERATURE: 0.5 # this randomness or creativity of the model's responses\n}"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define `credentials` and `project_id`,  which are necessary parameters to successfully run LLMs from watsonx.ai.\n",
    "\n",
    "(Keep `credentials` and `project_id` as they are now so that you do not need to create your own keys to run models. This supports you in running the model inside this lab environment. However, if you want to run the model locally, refer to this [tutorial](https://medium.com/the-power-of-ai/ibm-watsonx-ai-the-interface-and-api-e8e1c7227358) for creating your own keys.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Disclaimer\n",
    "This lab uses LLMs provided by **Watsonx.ai**. This environment has been configured to allow LLM use without API keys so you can prompt them for **free (with limitations)**. With that in mind, if you wish to run this notebook **locally outside** of Skills Network's JupyterLab environment, you will have to **configure your own API keys**. Please note that using your own API keys means that you will incur personal charges.\n",
    "\n",
    "### Running Locally\n",
    "If you are running this lab locally, you will need to configure your own API keys. This lab uses the `WatsonxLLM` module from `IBM`. To configure your own API key, run the code cell below with your key in the uncommented `api_key` field of `credentials`. **DO NOT** uncomment the `api_key` field if you aren't running locally, it will causes errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = {\n    \"url\": \"https://us-south.ml.cloud.ibm.com\"\n    # \"api_key\": \"your api key here\"\n    # uncomment above when running locally\n}\n\nproject_id = \"skills-network\""
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap the parameters to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n    model_id=model_id,\n    params=parameters,\n    credentials=credentials,\n    project_id=project_id\n)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a model called `flan_ul2_llm` from watsonx.ai.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "flan_ul2_llm = WatsonxLLM(model=model)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes the `LLM` part of the `Retrieval` task. <br>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/UZXQ44Tgv4EQ2-mTcu5e-A.png\" width=\"50%\" alt=\"split\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating LangChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain has a number of components that are designed to help retrieve information from the document and build question-answering applications, which helps you complete the `retrieve` part of the `Retrieval` task. <br>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/M4WpkkMMbfK0Wkz0W60Jiw.png\" width=\"50%\" alt=\"split\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following steps, you create a simple Q&A application over the document source using LangChain's `RetrievalQA`.\n",
    "\n",
    "Then, you ask the query \"what is mobile policy?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=flan_ul2_llm, \n                                 chain_type=\"stuff\", \n                                 retriever=docsearch.as_retriever(), \n                                 return_source_documents=False)\nquery = \"what is mobile policy?\"\nqa.invoke(query)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the response, it seems fine. The model's response is the relevant information about the mobile policy from the document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try to ask a more high-level question.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=flan_ul2_llm, \n                                 chain_type=\"stuff\", \n                                 retriever=docsearch.as_retriever(), \n                                 return_source_documents=False)\nquery = \"Can you summarize the document for me?\"\nqa.invoke(query)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this time, the model seems to not have the ability to summarize the document. This is because of the limitation of the `FLAN_UL2` model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, try to use another model, `LLAMA_3_70B_INSTRUCT`. You should do the model construction again.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'meta-llama/llama-3-3-70b-instruct'\n\nparameters = {\n    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,  \n    GenParams.MAX_NEW_TOKENS: 256,  # this controls the maximum number of tokens in the generated output\n    GenParams.TEMPERATURE: 0.5 # this randomness or creativity of the model's responses\n}\n\ncredentials = {\n    \"url\": \"https://us-south.ml.cloud.ibm.com\"\n}\n\nproject_id = \"skills-network\"\n\nmodel = Model(\n    model_id=model_id,\n    params=parameters,\n    credentials=credentials,\n    project_id=project_id\n)\n\nllama_3_llm = WatsonxLLM(model=model)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the same query again on this model.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llama_3_llm, \n                                 chain_type=\"stuff\", \n                                 retriever=docsearch.as_retriever(), \n                                 return_source_documents=False)\nquery = \"Can you summarize the document for me?\"\nqa.invoke(query)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you've created a simple Q&A application for your own document. Congratulations!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dive deeper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section dives deeper into how you can improve this application. You might want to ask \"How to add the prompt in retrieval using LangChain?\" <br>\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/bvw3pPRCYRUsv-Z2m33hmQ.png\" width=\"50%\" alt=\"split\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You use prompts to guide the responses from an LLM the way you want. For instance, if the LLM is uncertain about an answer, you instruct it to simply state, \"I do not know,\" instead of attempting to generate a speculative response.\n",
    "\n",
    "Let's see an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=flan_ul2_llm, \n                                 chain_type=\"stuff\", \n                                 retriever=docsearch.as_retriever(), \n                                 return_source_documents=False)\nquery = \"Can I eat in company vehicles?\"\nqa.invoke(query)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the query is asking something that does not exist in the document. The LLM responds with information that actually is not true. You don't want this to happen, so you must add a prompt to the LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using prompt template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, you create a prompt template using `PromptTemplate`.\n",
    "\n",
    "`context` and `question` are keywords in the RetrievalQA, so LangChain can automatically recognize them as document content and query.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the information from the document to answer the question at the end. If you don't know the answer, just say that you don't know, definately do not try to make up an answer.\n\n{context}\n\nQuestion: {question}\n\"\"\"\n\nPROMPT = PromptTemplate(\n    template=prompt_template, input_variables=[\"context\", \"question\"]\n)\n\nchain_type_kwargs = {\"prompt\": PROMPT}"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can ask the same question that does not have an answer in the document again.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llama_3_llm, \n                                 chain_type=\"stuff\", \n                                 retriever=docsearch.as_retriever(), \n                                 chain_type_kwargs=chain_type_kwargs, \n                                 return_source_documents=False)\n\nquery = \"Can I eat in company vehicles?\"\nqa.invoke(query)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the answer, you can see that the model responds with \"don't know\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the conversation have memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you want your conversations with an LLM to be more like a dialogue with a friend who remembers what you talked about last time? An LLM that retains the memory of your previous exchanges builds a more coherent and contextually rich conversation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at a situation in which an LLM does not have memory.\n",
    "\n",
    "You start a new query, \"What I cannot do in it?\". You do not specify what \"it\" is. In this case, \"it\" means \"company vehicles\" if you refer to the last query.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What I cannot do in it?\"\nqa.invoke(query)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the response, you see that the model does not have the memory because it does not provide the correct answer, which is something related to \"smoking is not permitted in company vehicles.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the LLM have memory, you introduce the `ConversationBufferMemory` function from LangChain.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key = \"chat_history\", return_message = True)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `ConversationalRetrievalChain` to retrieve information and talk with the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(llm=llama_3_llm, \n                                           chain_type=\"stuff\", \n                                           retriever=docsearch.as_retriever(), \n                                           memory = memory, \n                                           get_chat_history=lambda h : h, \n                                           return_source_documents=False)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `history` list to store the chat history.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is mobile policy?\"\nresult = qa.invoke({\"question\":query}, {\"chat_history\": history})\nprint(result[\"answer\"])"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append the previous query and answer to the history.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.append((query, result[\"answer\"]))"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"List points in it?\"\nresult = qa({\"question\": query}, {\"chat_history\": history})\nprint(result[\"answer\"])"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append the previous query and answer to the chat history again.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.append((query, result[\"answer\"]))"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the aim of it?\"\nresult = qa({\"question\": query}, {\"chat_history\": history})\nprint(result[\"answer\"])"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap up and make it an agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code defines a function to make an agent, which can retrieve information from the document and has the conversation memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa():\n    memory = ConversationBufferMemory(memory_key = \"chat_history\", return_message = True)\n    qa = ConversationalRetrievalChain.from_llm(llm=llama_3_llm, \n                                               chain_type=\"stuff\", \n                                               retriever=docsearch.as_retriever(), \n                                               memory = memory, \n                                               get_chat_history=lambda h : h, \n                                               return_source_documents=False)\n    history = []\n    while True:\n        query = input(\"Question: \")\n        \n        if query.lower() in [\"quit\",\"exit\",\"bye\"]:\n            print(\"Answer: Goodbye!\")\n            break\n            \n        result = qa({\"question\": query}, {\"chat_history\": history})\n        \n        history.append((query, result[\"answer\"]))\n        \n        print(\"Answer: \", result[\"answer\"])"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the function.\n",
    "\n",
    "Feel free to answer questions for your chatbot. For example: \n",
    "\n",
    "_What is the smoking policy? Can you list all points of it? Can you summarize it?_\n",
    "\n",
    "To **stop** the agent, you can type in 'quit', 'exit', 'bye'. Otherwise you cannot run other cells. \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa()"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have finished the project. Following are three exercises to help you to extend your knowledge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Work on your own document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are welcome to use your own document to practice. Another document has also been prepared that you can use for practice. Can you load this document and make the LLM read it for you? <br>\n",
    "Here is the URL to the document: https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/XVnuuEg94sAE4S_xAsGxBA.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for solution</summary>\n",
    "<br>\n",
    "    \n",
    "```python\n",
    "filename = 'stateOfUnion.txt'\n",
    "url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/XVnuuEg94sAE4S_xAsGxBA.txt'\n",
    "\n",
    "wget.download(url, out=filename)\n",
    "print('file downloaded')\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Return the source from the document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, you not only want the LLM to summarize for you, but you also want the model to return the exact content source from the document to you for reference. Can you adjust the code to make it happen?\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code "
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a hint</summary>\n",
    "All you must do is change the return_source_documents to True when you create the chain. And when you print, print the ['source_documents'][0] \n",
    "<br><br>\n",
    "\n",
    "    \n",
    "```python\n",
    "qa = RetrievalQA.from_chain_type(llm=llama_3_llm, chain_type=\"stuff\", retriever=docsearch.as_retriever(), return_source_documents=True)\n",
    "query = \"Can I smoke in company vehicles?\"\n",
    "results = qa.invoke(query)\n",
    "print(results['source_documents'][0]) ## this will return you the source content\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Use another LLM model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IBM watsonx.ai also has many other LLM models that you can use; for example, `mistralai/mixtral-8x7b-instruct-v01`, an open-source model from Mistral AI. Can you change the model to see the difference of the response?\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a hint</summary>\n",
    "\n",
    "To use a different LLM, go to the cell where the `model_id` is specified and replace the current `model_id` with the following code. Expect different results and performance when using different LLMs: \n",
    "\n",
    "```python\n",
    "model_id = 'mistralai/mixtral-8x7b-instruct-v01'\n",
    "```\n",
    "</br>\n",
    "\n",
    "After updating, run the remaining cells in the notebook to ensure the new model is used for subsequent operations.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Kang Wang](https://author.skills.network/instructors/kang_wang) <br>\n",
    "Kang Wang is a Data Scientist Intern in IBM. He is also a PhD Candidate in the University of Waterloo.\n",
    "\n",
    "[Faranak Heidari](https://www.linkedin.com/in/faranakhdr/) <br>\n",
    "Faranak Heidari is a Data Scientist Intern in IBM with a strong background in applied machine learning. Experienced in managing complex data to establish business insights and foster data-driven decision-making in complex settings such as healthcare. She is also a PhD candidate at the University of Toronto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Sina Nazeri](https://author.skills.network/instructors/sina_nazeri) <br>\n",
    "I am grateful to have had the opportunity to work as a Research Associate, Ph.D., and IBM Data Scientist. Through my work, I have gained experience in unraveling complex data structures to extract insights and provide valuable guidance.\n",
    "\n",
    "[Wojciech \"Victor\" Fulmyk](https://author.skills.network/instructors/wojciech_fulmyk) <br>\n",
    "Wojciech \"Victor\" Fulmyk is a Data Scientist at IBM and a Ph.D. candidate in Economics at the University of Calgary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{## Change Log}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{|Date (YYYY-MM-DD)|Version|Changed By|Change Description||-|-|-|-||2024-03-22|0.1|Kang Wang|Create the Project|}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "673bac911808ee65ef2666a50d3aad96f265adc4b405835468cddca25e1200cf"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
